# ğŸ½ï¸ Food RAG System

A Next.js application that implements Retrieval-Augmented Generation (RAG) for food-related queries. Features local development with Ollama and multiple deployment options for different environments.

## ğŸš€ Quick Start

### Option 1: Automated Setup (Windows)
```powershell
# Run the setup script in PowerShell
.\setup.ps1
npm run dev
```

### Option 2: Manual Setup
```bash
# Install dependencies
npm install

# Copy environment configuration
cp .env.simple.example .env.local

# Download Ollama models (takes several minutes)
ollama pull mxbai-embed-large
ollama pull llama3.2

# Start development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and start asking questions about food!

ğŸ“– **For detailed setup instructions, see [LOCAL_SETUP.md](LOCAL_SETUP.md)**

## ğŸ¯ How It Works

1. **Load Data**: Click "Embed New Items" to process the food database
2. **Ask Questions**: Type questions like "What fruits are yellow and sweet?"
3. **See Results**: 
   - Your question appears immediately
   - RAG search results show first (relevant food documents)
   - AI response generates using the retrieved context

## ğŸ—ï¸ Architecture Options

### Upstash Vector Database (Recommended)
```bash
# .env.local - Production Ready
VECTOR_DB_TYPE=upstash
UPSTASH_VECTOR_URL=your_upstash_url
UPSTASH_VECTOR_TOKEN=your_upstash_token
LLM_PROVIDER=groq
```
âœ… Built-in embeddings via mixedbread-ai/mxbai-embed-large-v1
âœ… Scalable and persistent storage
âœ… Ready for Vercel deployment

### Simple Vector Database (Development)
```bash
# .env.local - Quick Local Development
VECTOR_DB_TYPE=simple
LLM_PROVIDER=ollama
```
âœ… Works immediately, no setup required
âœ… In-memory storage for testing

### ChromaDB (Legacy)
```bash
# .env.local - Advanced Local Setup
VECTOR_DB_TYPE=chroma
LLM_PROVIDER=ollama
```
âš ï¸ Requires additional setup

## ğŸ—ï¸ Architecture

### File Structure
```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â”œâ”€â”€ data-actions.ts      # Food data loading
â”‚   â”‚   â”œâ”€â”€ embedding-actions.ts # Embedding operations
â”‚   â”‚   â””â”€â”€ query-actions.ts     # RAG queries
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx                 # Main UI
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ EmbeddingSection.tsx     # Embedding status & controls
â”‚   â”œâ”€â”€ QuestionSection.tsx      # Question input form
â”‚   â””â”€â”€ ResponseSection.tsx      # Results display
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ database.ts          # Vector DB abstraction
â”‚   â”‚   â”œâ”€â”€ embeddings.ts        # Embedding services
â”‚   â”‚   â””â”€â”€ llm.ts              # LLM services
â”‚   â”œâ”€â”€ config.ts               # Environment configuration
â”‚   â””â”€â”€ types.ts                # TypeScript interfaces
â”œâ”€â”€ foods.json                  # Food data
â””â”€â”€ .env.local                  # Environment variables
```

### Provider Architecture
- **Database Layer**: ChromaDB â†” Upstash Vector
- **Embedding Layer**: Ollama â†” Clarifai
- **LLM Layer**: Ollama â†” Groq
- **Factory Pattern**: Environment-based provider switching

## ğŸš€ Deployment

### Phase 2 - Vercel Cloud Deployment

1. **Set up cloud services:**
   - Create Upstash Vector database
   - Get Clarifai API key
   - Get Groq API key

2. **Update environment variables:**
   ```bash
   VECTOR_DB_TYPE=upstash
   EMBEDDING_PROVIDER=clarifai
   LLM_PROVIDER=groq
   UPSTASH_VECTOR_URL=your_upstash_url
   UPSTASH_VECTOR_TOKEN=your_upstash_token
   CLARIFAI_PAT=your_clarifai_key
   CLARIFAI_MODEL_URL=https://clarifai.com/mixedbread-ai/embed/models/mxbai-embed-large-v1
   GROQ_API_KEY=your_groq_key
   GROQ_MODEL=llama-3.2-3b-preview
   ```

3. **Deploy to Vercel:**
   ```bash
   npm run build
   vercel --prod
   ```

## ğŸ”§ Configuration

### Provider Configuration
Change these environment variables to configure your setup:

#### Vector Database (Default: Upstash)
- `VECTOR_DB_TYPE`: `upstash` (recommended) | `simple` (development) | `chroma` (legacy)
- `UPSTASH_VECTOR_URL`: Your Upstash Vector instance URL
- `UPSTASH_VECTOR_TOKEN`: Your Upstash authentication token

#### LLM Provider (Required)
- `LLM_PROVIDER`: `groq` (recommended) | `ollama` (local development)
- `GROQ_API_KEY`: Your Groq API key
- `GROQ_MODEL`: Default is llama-3.2-3b-preview

Note: Embedding is handled automatically by Upstash using mixedbread-ai/mxbai-embed-large-v1

## ğŸ“Š Data Format

Food items in `foods.json`:
```json
{
  "id": "1",
  "text": "A banana is a yellow fruit that is soft and sweet.",
  "region": "Tropical",
  "type": "Fruit"
}
```

## ğŸ› Troubleshooting

### Common Issues

1. **Ollama Connection Failed**
   - Ensure Ollama is running: `ollama serve`
   - Check models are pulled: `ollama list`

2. **ChromaDB Permission Errors**
   - Check `CHROMA_PATH` directory permissions
   - Try different path if needed

3. **Rate Limiting (Cloud)**
   - Use free tier responsibly
   - Implement retry logic if needed

4. **Build Errors**
   - Check TypeScript errors: `npm run build`
   - Verify all dependencies: `npm install`

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request
